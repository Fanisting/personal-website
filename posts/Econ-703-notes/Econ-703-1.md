---
title: "Lecture Note: Econ 703 (week 1)"
description: Note about PhD-level Econometric course
date: 2023-08-29
tags:
  - English
---

> Date: 2023.08.29
>
> This class focus on **modes of convergence**.
>
> - almost sure convergence
>   - defined by $P(\lim)$
>
> - convergence in probability
>   - defined by $\lim(P)$
> - convergence in distribution/week convergence
>   - defined by CDF or probability in set $P(X_n \in A)$

  

## Modes of Convergence

### Probability Space

Definition a probability space by $(\Omega, \mathcal{F}, \mathcal{P})$, and $X_n, \Omega \rightarrow \mathcal{R}$ 

### Sure convergence

$$
\lim_{n\to\infty}X_n(\omega) = X(\omega), \text{for any } \omega \in \Omega
$$

### almost sure convergence

Define as $X_n\overset{a.s.}{\operatorname*{\to}}X$. 

Suppose we have a random variable $X_n$ and $X$, then almost sure convergence means:
$$
\mathbb{P}(\lim_{n\to\infty}X_n=X)=1
\\
or
\\
\mathbb{P}(\lim_{n\to\infty}d(X_n,X)=0)=1
$$
**Note**: strongest one, and defined on probability is nearly the same everywhere.

### Convergence in probability

Define $X_n\overset{p}{\operatorname*{\to}}X$ as $X_n$ converges in probability to $X$, and the key is the probability of limit. 
$$
\lim_{n\to\infty}\mathbb{P}(|X_n - X| \leq \epsilon) = 0, \text{for any } \epsilon
$$


#### Properties

- Convergence in probability → Convergence in distribution

- Convergence in distribution → Convergence in probability when X = c (a constant)

- [**continuous mapping theorem**] for any continuous function $g$:
  $$
  X_n\overset{p}{\operatorname*{\to}}X, g(X_n)\overset{p}{\operatorname*{\to}}g(X)
  $$

- For $X_n\overset{p}{\operatorname*{\to}}X, Y_n\overset{p}{\operatorname*{\to}}Y$, we can have $X_n + Y_n \overset{p}{\operatorname*{\to}} X + Y$ 

  - in some special cases, it applies for conv in distribution
  - if $X_n, Y_n$ are independent, we have $X_n - Y_n \overset{p}{\operatorname*{\to}} X - Y$ 

### Convergence in distribution

Other names can be **converge weakly/converge in law**. 
$$
F_{X_n}(x) \to F(x)
$$

#### Properties

- **Portmanteau lemma**

  following two arguments are same:

$$
X_n\overset{d}{\operatorname*{\to}}X
\\
\lim_{n \to \infty} \mathbb{P}(X_n \in A) = \mathbb{P}(X \in A)
$$

- **Continuous mapping theorem**

  for any continuous function $g$:
  $$
  X_n\overset{d}{\operatorname*{\to}}X, g(X_n)\overset{d}{\operatorname*{\to}}g(X)
  $$





### Other: Convergence in mean

Define $X_n$ converges in $r^{th}$ mean towards the random variable $X$ if and only if:

$$
\lim_{n\to\infty}\mathbb{E}\left(|X_n-X|^r\right)=0
$$
So some special cases are:

- when $r = 1$, we say $X_n$ converges in **mean** to $X$
- when $r=2$, we say $X_n$ converges in **mean square** to $X$

### Theorem from Marginal Joint distribution

For $X_n\overset{p}{\operatorname*{\to}}X, Y_n\overset{p}{\operatorname*{\to}}Y$, we have marginal joint distribution $(X_n, Y_n) \overset{p}{\operatorname*{\to}} (X, Y)$

But when it comes to converges in distribution, we need extra assumptions:

- For $X_n\overset{p}{\operatorname*{\to}}X, Y_n\overset{p}{\operatorname*{\to}}C$, we can have $(X_n, Y_n) \overset{d}{\operatorname*{\to}} (X, C)$



## Stochastic Order Notation

### Big O: stochastic boundedness

$X_n = O(r_n)$ $|\frac{X_n}{r_n}| < M \in C$ iff
$$
P\left(|\frac{X_n}{a_n}|>M\right)<\varepsilon,\mathrm{~}\forall\mathrm{~}n>N
$$
which means $X_n$ is stochastically bounded. 

### Small o: convergence in probability

$X_n = o(r_n)$ iff  
$$
\lim_{n\to\infty}P(|\frac{X_n}{r_n}| \geq \epsilon) = 0
$$
 for every positive $\epsilon$, which means that $r_n$ increase much faster than $X_n$ in any time. 

### Theorems

- $o_p(1) + o_p(1) = o_p(1)$
- $o_p(1) + O_p(1) = O_p(1)$
- $o_p(1)O_p(1) = o_p(1)$



## LLNs

This section covers several variants of LLM(Law of Large numbers).

Here we use notation as follows:

- $\{X_i\}$ is a series of random vars $\{X_1, X_2, ... X_n\}$
- Denote $\mathbb{E}$ as mean of var, and denote $var(X_i)$ as variance
- i.i.d: identical and independent distribution
- Different convergence: $\overset{d}{\operatorname*{\to}}, \overset{p}{\operatorname*{\to}}$ 

### Theorem(1713)

Given $\{X_i\} \overset{i.i.d}{\operatorname*{\to}}Bernoulli(p)$, then $\overline{X_n} \overset{d}{\operatorname*{\to}} p$

- Note: a special case for LLN.

### Forms of Theorems

when we wanna find a specific theorem, we should define:

**Assumptions**

- independent vars
- allows distribution not change

**Conclusions**

- convergence in p (weak) or in a.s. (strong)
- shape of tails (fat/not fat)

### Theorem (pre Chebyshev weak LLN)

For $\{X_i\}$ all independent (not i.i.d), we have $\mathbb{E}(X-i) = \mu < \infty$ and $var(X_i) \leq M$ for all $i$, then there exists $0<M<\infty$ such that:
$$
\overline{X_n} \overset{p}{\operatorname*{\to}} \mu
$$

- Assumption about variance can be relaxed to: $\frac{1}{n^2}\sum_ivar(X_i) \leq o(1)$
- Note: when random var $X_i$ is not unusual (infinite variance), averaging converges to expectation.

**Proof**

- Given $\mathbb{E}(X_i) = \mu$, we know $\mathbb{E}(\overline{X}) = \mu$

- use property: $P(|\overline{X_n} - \mathbb{E}(X_n)| \geq \epsilon) \leq \frac{var(\overline{X_n})}{\epsilon^2}$

  from a bounded variance, we can know:
  $$
  P(|\overline{X_n} - \mathbb{E}(X_n)| \geq \epsilon) \leq \frac{var(\overline{X_n})}{\epsilon^2} \leq \frac{1}{\epsilon^2}\frac{M}{n}
  $$

- According to definition of convergence in p, when $n \to \infty$, we ensure that $\lim_{n \to \infty}P(|\overline{X_n} - \mathbb{E}(X_n)| \geq \epsilon) \leq \lim_{n \to \infty}\frac{1}{\epsilon^2}\frac{M}{n} = 0$



### Theorem (Kolmogorov's 2nd strong LLN)

Given $\{X_i\}$ are i.i.d, then
$$
\overline{X_n} \to \mu \quad \text{a.s.}
$$
iff $\mathbb{E}(X_i)$ exists and equals to $\mu$ **for all i**

- for i.i.d sequence, not all finite variance for all vars, no a.s. convergence



## CLTs

This section covers several variants of CLT(Central Limit Theorem).

From LLN we know:
$$
(\overline{X_n} - \mu) \overset{p}{\operatorname*{\to}} 0
$$
Now we wanna know the "shape" of such convergence, which is about asymptotic distribution/density.

An intuitive approach is to add a **scaler** $f(n)$ to enlarge the item:
$$
f(n)(\overline{X_n} - \mu) \overset{p}{\operatorname*{\to}} ?
$$
CLT implies that for a special scaler:
$$
f(n) = \sqrt{n}
$$


we have a magical property that we can have a special distribution $Z$:
$$
\sqrt{n}(\overline{X_n} - \mu) \overset{d}{\operatorname*{\to}} Z
$$

### Theorem (Lindeberg-Lévy CLT)

Given:

- $\{X_i\}$ are i.i.d
- $\mathbb{E}(X_i) = \mu$ and $var(X_i) = \sigma^2$

then we have:
$$
\sqrt{n}(\frac{\overline{X_n} - \mu}{\sigma}) \overset{d}{\operatorname*{\to}} N(0,1)
$$

- Note: a special scaler results in a special distribution, and it is **robust for all random vars**

### Theorem (Cramér–Wold, vector-form)

The above theorem can be easily generalized to vector form. 

Following are equal:

- $\mathbb{X}_n \overset{d}{\operatorname*{\to}} \mathbb{X}$
- $\lambda'\mathbb{X}_n \overset{d}{\operatorname*{\to}} \lambda'\mathbb{X}$ for all $\lambda \in R^k$

So $\lambda'\mathbb{X}_n$ is the linear combination of the vector of random vars. 

### Theorem (multi-var form)

- $\{X_i\}$ are i.i.d
- $\mathbb{E}(X_i) = \mu$ and $var(X_i) = v$

then we have:
$$
\sqrt{n}(\frac{\overline{X_n} - \mu}{\sigma}) \overset{d}{\operatorname*{\to}} N(0,v)
$$

### Theorem (Berry-Esseen)

let $J_n(t) = P(\sqrt{n}(\frac{\overline{X_n} - \mu}{\sigma}) \leq t)$, which is our targeted CDF.

let $\{X_i\}$ be i.i.d with finite 3rd moment, then there exists constant $c$ such that:
$$
\|J_{n}-\Phi\|_{\infty} \leq c \frac{\mathbb{E}(|X-\mathbb{E}(X)|)^3}{var(X)^{3/2}\sqrt{}n}
$$

- Note: our target CDF is bounded, and generally we can find a small 

